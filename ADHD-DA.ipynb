{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load and Preprocess EEG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load .mat files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "def load_mat_files(directory):\n",
    "    \"\"\"Load all .mat files from a directory into a dictionary\"\"\"\n",
    "    data_dict = {}\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} not found\")\n",
    "        return data_dict\n",
    "        \n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.mat'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                # Load the .mat file\n",
    "                mat_data = scipy.io.loadmat(file_path)\n",
    "                # Get the variable name from the file name \n",
    "                var_name = file.split('.')[0]\n",
    "                # Extract the EEG data using the variable name as key\n",
    "                eeg_data = mat_data[var_name]\n",
    "                # Store in dictionary with filename as key\n",
    "                data_dict[file] = eeg_data\n",
    "                print(f\"Successfully loaded {file} with shape {eeg_data.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {str(e)}\")\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess EEG Data\n",
    "1. Bandpass filter (1-50 Hz)\n",
    "2. ICA for artifact removal \n",
    "3. Epoching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_eeg(raw_data, sfreq=128):\n",
    "    \"\"\"\n",
    "    Preprocess single subject EEG data with:\n",
    "    1. Bandpass filter (1-50 Hz)\n",
    "    2. ICA for artifact removal\n",
    "    3. Epoching\n",
    "    \"\"\"\n",
    "    # Create MNE Raw object\n",
    "    ch_names = [f'EEG{i+1}' for i in range(raw_data.shape[1])]\n",
    "    ch_types = ['eeg'] * raw_data.shape[1]\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(raw_data.T, info)\n",
    "    \n",
    "    try:\n",
    "        # Bandpass filter (1-50 Hz)\n",
    "        raw_filtered = raw.copy().filter(l_freq=1, h_freq=50)\n",
    "        \n",
    "        # ICA for artifact removal\n",
    "        ica = ICA(\n",
    "            n_components=min(raw_data.shape[1] - 1, 15),\n",
    "            random_state=42,\n",
    "            max_iter=800\n",
    "        )\n",
    "        ica.fit(raw_filtered)\n",
    "        raw_clean = raw_filtered.copy()\n",
    "        ica.apply(raw_clean)\n",
    "        \n",
    "        #Create epochs (2-second epochs with 50% overlap)\n",
    "        events = mne.make_fixed_length_events(\n",
    "            raw_clean,\n",
    "            duration=2.0,\n",
    "            overlap=1.0\n",
    "        )\n",
    "        \n",
    "        epochs = mne.Epochs(\n",
    "            raw_clean,\n",
    "            events,\n",
    "            tmin=0,\n",
    "            tmax=2,\n",
    "            baseline=None,\n",
    "            preload=True\n",
    "        )\n",
    "        \n",
    "        epoched_data = epochs.get_data()\n",
    "        print(f\"Successfully preprocessed data: {epoched_data.shape}\")\n",
    "        return epoched_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction \n",
    "Extract 3 features \n",
    "1. PSD \n",
    "(Delta (1-4 Hz)\n",
    "Theta (4-8 Hz)\n",
    "Alpha (8-13 Hz)\n",
    "Beta (13-30 Hz)\n",
    "Gamma (30-45 Hz))\n",
    "2. Theta/Beta Ratio\n",
    "3. Higuchi Fractal Dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_features(epoched_data, sfreq=128):\n",
    "    \"\"\"\n",
    "    Extract PSD, Theta/Beta Ratio, and Fractal Dimension features from epoched data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epoched_data : numpy.ndarray\n",
    "        Epoched data of shape (n_epochs, n_channels, n_timepoints)\n",
    "    sfreq : float\n",
    "        Sampling frequency\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of dictionaries containing features for all epochs\n",
    "    \"\"\"\n",
    "    n_epochs, n_channels, n_timepoints = epoched_data.shape\n",
    "    all_features = []\n",
    "    \n",
    "    # Process each epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_features = {}\n",
    "        \n",
    "        # Process each channel\n",
    "        for channel in range(n_channels):\n",
    "            # Get channel data for current epoch\n",
    "            data = epoched_data[epoch, channel]\n",
    "            \n",
    "            #Calculate PSD using Welch's method\n",
    "            freqs, psd = signal.welch(data, fs=sfreq, nperseg=min(256, n_timepoints))\n",
    "            \n",
    "            # Calculate power in different frequency bands\n",
    "            bands = {\n",
    "                'delta': (1, 4),\n",
    "                'theta': (4, 8),\n",
    "                'alpha': (8, 13),\n",
    "                'beta': (13, 30),\n",
    "                'gamma': (30, 45)\n",
    "            }\n",
    "            \n",
    "            for band_name, (fmin, fmax) in bands.items():\n",
    "                # Find frequencies within band\n",
    "                mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "                # Calculate average power in band\n",
    "                power = np.mean(psd[mask])\n",
    "                epoch_features[f'{band_name}_power_ch{channel}'] = power\n",
    "            \n",
    "            # Calculate Theta/Beta Ratio\n",
    "            theta_mask = (freqs >= 4) & (freqs <= 8)\n",
    "            beta_mask = (freqs >= 13) & (freqs <= 30)\n",
    "            theta_power = np.mean(psd[theta_mask])\n",
    "            beta_power = np.mean(psd[beta_mask])\n",
    "            theta_beta_ratio = theta_power / beta_power if beta_power != 0 else 0\n",
    "            epoch_features[f'theta_beta_ratio_ch{channel}'] = theta_beta_ratio\n",
    "            \n",
    "            # Calculate Higuchi Fractal Dimension\n",
    "            def calculate_hfd(signal_data, kmax=10):\n",
    "                N = len(signal_data)\n",
    "                L = []\n",
    "                x = []\n",
    "                \n",
    "                for k in range(1, kmax + 1):\n",
    "                    Lk = 0\n",
    "                    for m in range(k):\n",
    "                        # Calculate length Lm(k)\n",
    "                        Lmk = 0\n",
    "                        for i in range(1, int((N - m) / k)):\n",
    "                            Lmk += abs(signal_data[m + i * k] - signal_data[m + (i - 1) * k])\n",
    "                        Lmk = Lmk * (N - 1) / (((N - m) / k) * k)\n",
    "                        Lk += Lmk\n",
    "                    L.append(np.log(Lk / k))\n",
    "                    x.append(np.log(1.0 / k))\n",
    "                \n",
    "                # Fit line and get slope\n",
    "                p = np.polyfit(x, L, 1)\n",
    "                return -p[0]  # Fractal dimension is the negative of the slope\n",
    "            \n",
    "            hfd = calculate_hfd(data)\n",
    "            epoch_features[f'fractal_dim_ch{channel}'] = hfd\n",
    "            \n",
    "        all_features.append(epoch_features)\n",
    "    \n",
    "    return all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ADHD group data...\n",
      "Directory ADHD_part1 not found\n",
      "Directory ADHD_part2 not found\n",
      "\n",
      "Loading Control group data...\n",
      "Directory Control_part1 not found\n",
      "Directory Control_part2 not found\n",
      "\n",
      "Processing ADHD data...\n",
      "\n",
      "Processing Control data...\n",
      "\n",
      "Feature extraction complete!\n",
      "Total samples: 0\n",
      "Number of features: 0\n",
      "Features saved to 'eeg_features.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    adhd_part1_path = Path('ADHD_part1')\n",
    "    adhd_part2_path = Path('ADHD_part2')\n",
    "    control_part1_path = Path('Control_part1')\n",
    "    control_part2_path = Path('Control_part2')\n",
    "    \n",
    "    # Load all data\n",
    "    print(\"Loading ADHD group data...\")\n",
    "    adhd_data_part1 = load_mat_files(adhd_part1_path)\n",
    "    adhd_data_part2 = load_mat_files(adhd_part2_path)\n",
    "\n",
    "    print(\"\\nLoading Control group data...\")\n",
    "    control_data_part1 = load_mat_files(control_part1_path)\n",
    "    control_data_part2 = load_mat_files(control_part2_path)\n",
    "    \n",
    "    # Initialize feature storage\n",
    "    processed_features = {\n",
    "        'adhd': [],\n",
    "        'control': []\n",
    "    }\n",
    "    \n",
    "    # Process ADHD data\n",
    "    print(\"\\nProcessing ADHD data...\")\n",
    "    for data_dict in [adhd_data_part1, adhd_data_part2]:\n",
    "        for file_name, eeg_data in data_dict.items():\n",
    "            try:\n",
    "                # First preprocess the data\n",
    "                processed = preprocess_eeg(eeg_data)\n",
    "                if processed is not None:\n",
    "                    # Then extract features\n",
    "                    features = extract_features(processed)\n",
    "                    processed_features['adhd'].extend(features)\n",
    "                    print(f\"Successfully processed and extracted features from {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {str(e)}\")\n",
    "    \n",
    "    # Process Control data\n",
    "    print(\"\\nProcessing Control data...\")\n",
    "    for data_dict in [control_data_part1, control_data_part2]:\n",
    "        for file_name, eeg_data in data_dict.items():\n",
    "            try:\n",
    "                # First preprocess the data\n",
    "                processed = preprocess_eeg(eeg_data)\n",
    "                if processed is not None:\n",
    "                    # Then extract features\n",
    "                    features = extract_features(processed)\n",
    "                    processed_features['control'].extend(features)\n",
    "                    print(f\"Successfully processed and extracted features from {file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    adhd_df = pd.DataFrame(processed_features['adhd'])\n",
    "    control_df = pd.DataFrame(processed_features['control'])\n",
    "    \n",
    "    # Add labels\n",
    "    adhd_df['label'] = 1\n",
    "    control_df['label'] = 0\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = pd.concat([adhd_df, control_df], axis=0)\n",
    "    all_features = all_features.reset_index(drop=True)\n",
    "    \n",
    "    # Save features\n",
    "    all_features.to_csv('eeg_features.csv', index=False)\n",
    "    \n",
    "    print(\"\\nFeature extraction complete!\")\n",
    "    print(f\"Total samples: {len(all_features)}\")\n",
    "    print(f\"Number of features: {len(all_features.columns) - 1}\")  # Excluding label column\n",
    "    print(\"Features saved to 'eeg_features.csv'\")\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features_df = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM Classifier \n",
    "1. ~92.5% accuracy \n",
    "2. Evaluatio?n: cross valication, confusion matrix, performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from eeg_features.csv...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 109\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel and scaler saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 109\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[12], line 95\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Train and evaluate the classifier\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     results \u001b[38;5;241m=\u001b[39m train_svm_classifier()\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Print summary\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mtrain_svm_classifier\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m y \u001b[38;5;241m=\u001b[39m features_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     24\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Scale the features\u001b[39;00m\n\u001b[1;32m     28\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2619\u001b[0m )\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2270\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_svm_classifier():\n",
    "    \"\"\"\n",
    "    Train SVM classifier on the EEG features\n",
    "    \"\"\"\n",
    "    # Load the features\n",
    "    print(\"Loading features from eeg_features.csv...\")\n",
    "    features_df = pd.read_csv('eeg_features.csv')\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = features_df.drop('label', axis=1)\n",
    "    y = features_df['label']\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Initialize and train SVM\n",
    "    print(\"\\nTraining SVM classifier...\")\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate using cross-validation\n",
    "    print(\"\\nPerforming cross-validation...\")\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv=cv)\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Average CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Control', 'ADHD']))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Control', 'ADHD'],\n",
    "                yticklabels=['Control', 'ADHD'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    print(\"\\nAnalyzing feature importance...\")\n",
    "    if svm.kernel == 'linear':\n",
    "        # For linear kernel, we can directly get feature importance\n",
    "        importance = np.abs(svm.coef_[0])\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': importance\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot top 20 most important features\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(data=feature_importance.head(20), x='importance', y='feature')\n",
    "        plt.title('Top 20 Most Important Features')\n",
    "        plt.xlabel('Absolute Coefficient Value')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': svm,\n",
    "        'scaler': scaler,\n",
    "        'cv_scores': cv_scores,\n",
    "        'test_accuracy': svm.score(X_test_scaled, y_test),\n",
    "        'classification_report': classification_report(y_test, y_pred),\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Train and evaluate the classifier\n",
    "    results = train_svm_classifier()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Average CV accuracy: {results['cv_scores'].mean():.3f}\")\n",
    "    print(f\"Test set accuracy: {results['test_accuracy']:.3f}\")\n",
    "    \n",
    "    # Save the model and scaler\n",
    "    import joblib\n",
    "    joblib.dump(results['model'], 'svm_model.joblib')\n",
    "    joblib.dump(results['scaler'], 'scaler.joblib')\n",
    "    print(\"\\nModel and scaler saved to 'svm_model.joblib' and 'scaler.joblib'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
